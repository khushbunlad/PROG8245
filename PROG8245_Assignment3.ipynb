{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3: Exploring IR & NLP\n",
    "- In this assignment we are going to implement various IR techniques <b><i>From Scratch</i></b>, Please don't use available libraries except if specified that you can use it.\n",
    "- You are required to submit 6 different functions for this assignment, you can additional helper functions but only 6 will be tested.\n",
    "- You will be granted 10 marks for clean code and documenting the code.\n",
    "- Student Name: **Khushbu Nileshkumar Lad**\n",
    "- ID: **9027375**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentences = [\n",
    "    \"Python is a versatile programming language, python proved its importance in various domains.\",\n",
    "    \"JavaScript is widely used for web development.\",\n",
    "    \"Java is known for its platform independence.\",\n",
    "    \"Programming involves writing code to solve problems.\",\n",
    "    \"Data structures are crucial for efficient programming.\",\n",
    "    \"Algorithms are step-by-step instructions for solving problems.\",\n",
    "    \"Version control systems help manage code changes in collaboration.\",\n",
    "    \"Debugging is the process of finding and fixing errors in python code.\",\n",
    "    \"Web frameworks simplify the development of web applications.\",\n",
    "    \"Artificial intelligence can be applied in various programming tasks.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Khushbu.Lad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Khushbu.Lad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Khushbu.Lad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\Khushbu.Lad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Khushbu.Lad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "nltk.download('words')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART A: Preprocessing (15 Marks)\n",
    "- You are required to preprocess the text and apply the tokenization process.<br/>\n",
    "- Proprocessing should include tokenization, normalization, stemming <b>OR</b> lemmatization, and Named Entity Recognition (NER).<br/>\n",
    "- You need to make sure that Named Entities are not broken into separate tokens, but should be normalized by case-folding only. <br/>\n",
    "- The output of this step should be list of tokenized sentences. [[sentence1_token1, sentence1_token2, .. .], [sentence2_token1, .. .], .. .] <br/>\n",
    "- Please write the functionality of clean_sentences as explained in the comment (Please do comment your code at each essential step) <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\Khushbu.Lad\\AppData\\Local\\Temp\\ipykernel_11648\\916178760.py:6: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  special_characters = [',','.','!','\"','#','$','%','&','_','(',')','*','+','/',':','','<','=','>','@','[','\\\\',']','^','`','{','|','}','~','\\t', '\\s']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['python',\n",
       "  'versatil',\n",
       "  'program',\n",
       "  'languag',\n",
       "  'python',\n",
       "  'prove',\n",
       "  'import',\n",
       "  'variou',\n",
       "  'domain'],\n",
       " ['javascript', 'wide', 'use', 'web', 'develop'],\n",
       " ['java', 'known', 'platform', 'independ'],\n",
       " ['program', 'involv', 'write', 'code', 'solv', 'problem'],\n",
       " ['data', 'structur', 'crucial', 'effici', 'program'],\n",
       " ['algorithm', 'step-by-step', 'instruct', 'solv', 'problem'],\n",
       " ['version',\n",
       "  'control',\n",
       "  'system',\n",
       "  'help',\n",
       "  'manag',\n",
       "  'code',\n",
       "  'chang',\n",
       "  'collabor'],\n",
       " ['debug', 'process', 'find', 'fix', 'error', 'python', 'code'],\n",
       " ['web', 'framework', 'simplifi', 'develop', 'web', 'applic'],\n",
       " ['artifici', 'intellig', 'appli', 'variou', 'program', 'task']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## You are allowed for PART A to use any library that would help you in the task.\n",
    "def clean_sentences(sentences=None):\n",
    "    ## This function takes as an input list of sentences\n",
    "    ## This function returns a list of tokenized_sentences\n",
    "    \n",
    "    special_characters = [',','.','!','\"','#','$','%','&','_','(',')','*','+','/',':','','<','=','>','@','[','\\\\',']','^','`','{','|','}','~','\\t', '\\s']\n",
    "    stemmer = PorterStemmer()\n",
    "    # Output array\n",
    "    all_tokens = []\n",
    "    # Create and append array for each sentence\n",
    "    for sen in sentences:\n",
    "        # Tokenize words in sentence\n",
    "        words = word_tokenize(sen)\n",
    "\n",
    "        # Eliminate stop words\n",
    "        words = [word for word in words if word not in stopwords.words('english')]\n",
    "\n",
    "        # Eliminate special characters\n",
    "        words = [word for word in words if word not in special_characters]\n",
    "\n",
    "        # Apply stemming on tokens\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "        \n",
    "        # Append this sentence's tokens to all token array\n",
    "        all_tokens.append(words)\n",
    "\n",
    "    return all_tokens\n",
    "\n",
    "clean_sentences(sample_sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART B: Building IR Sentence-Word Representation (30 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Question B-1: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the <b>inverted index</b> that is sufficient to represent the document. Assume that each sentence is a document and the sentence ID starts from 1. (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'python': [1, 8],\n",
       " 'versatil': [1],\n",
       " 'program': [1, 4, 5, 10],\n",
       " 'languag': [1],\n",
       " 'prove': [1],\n",
       " 'import': [1],\n",
       " 'variou': [1, 10],\n",
       " 'domain': [1],\n",
       " 'javascript': [2],\n",
       " 'wide': [2],\n",
       " 'use': [2],\n",
       " 'web': [2, 9],\n",
       " 'develop': [2, 9],\n",
       " 'java': [3],\n",
       " 'known': [3],\n",
       " 'platform': [3],\n",
       " 'independ': [3],\n",
       " 'involv': [4],\n",
       " 'write': [4],\n",
       " 'code': [4, 7, 8],\n",
       " 'solv': [4, 6],\n",
       " 'problem': [4, 6],\n",
       " 'data': [5],\n",
       " 'structur': [5],\n",
       " 'crucial': [5],\n",
       " 'effici': [5],\n",
       " 'algorithm': [6],\n",
       " 'step-by-step': [6],\n",
       " 'instruct': [6],\n",
       " 'version': [7],\n",
       " 'control': [7],\n",
       " 'system': [7],\n",
       " 'help': [7],\n",
       " 'manag': [7],\n",
       " 'chang': [7],\n",
       " 'collabor': [7],\n",
       " 'debug': [8],\n",
       " 'process': [8],\n",
       " 'find': [8],\n",
       " 'fix': [8],\n",
       " 'error': [8],\n",
       " 'framework': [9],\n",
       " 'simplifi': [9],\n",
       " 'applic': [9],\n",
       " 'artifici': [10],\n",
       " 'intellig': [10],\n",
       " 'appli': [10],\n",
       " 'task': [10]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_inverted_index(list_of_sentence_tokens):\n",
    "    ## TODO: Implement the functionality that will return the inverted index\n",
    "\n",
    "    inverted_index = {}\n",
    "    # Iterate through the sentences with their IDs\n",
    "    for sentence_index, tokens in enumerate(list_of_sentence_tokens, start=1):\n",
    "        # Iterate through each token in the sentence\n",
    "        for token in tokens:\n",
    "            # Add the sentence ID to the token's list in the inverted index\n",
    "            if token in inverted_index:\n",
    "                if sentence_index not in inverted_index[token]:\n",
    "                    inverted_index[token].append(sentence_index)\n",
    "            else:\n",
    "                inverted_index[token] = [sentence_index]\n",
    "    \n",
    "    return inverted_index\n",
    "get_inverted_index(clean_sentences(sample_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Question B-2: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the <b>Positional index</b> that is sufficient to represent the document. Assume that each sentence is a document and the sentence ID starts from 1, and the first token in the list is at position 0. Make sure to consider multiple appearance of the same token. (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'python': {1: [0, 4], 8: [5]},\n",
       " 'versatil': {1: [1]},\n",
       " 'program': {1: [2], 4: [0], 5: [4], 10: [4]},\n",
       " 'languag': {1: [3]},\n",
       " 'prove': {1: [5]},\n",
       " 'import': {1: [6]},\n",
       " 'variou': {1: [7], 10: [3]},\n",
       " 'domain': {1: [8]},\n",
       " 'javascript': {2: [0]},\n",
       " 'wide': {2: [1]},\n",
       " 'use': {2: [2]},\n",
       " 'web': {2: [3], 9: [0, 4]},\n",
       " 'develop': {2: [4], 9: [3]},\n",
       " 'java': {3: [0]},\n",
       " 'known': {3: [1]},\n",
       " 'platform': {3: [2]},\n",
       " 'independ': {3: [3]},\n",
       " 'involv': {4: [1]},\n",
       " 'write': {4: [2]},\n",
       " 'code': {4: [3], 7: [5], 8: [6]},\n",
       " 'solv': {4: [4], 6: [3]},\n",
       " 'problem': {4: [5], 6: [4]},\n",
       " 'data': {5: [0]},\n",
       " 'structur': {5: [1]},\n",
       " 'crucial': {5: [2]},\n",
       " 'effici': {5: [3]},\n",
       " 'algorithm': {6: [0]},\n",
       " 'step-by-step': {6: [1]},\n",
       " 'instruct': {6: [2]},\n",
       " 'version': {7: [0]},\n",
       " 'control': {7: [1]},\n",
       " 'system': {7: [2]},\n",
       " 'help': {7: [3]},\n",
       " 'manag': {7: [4]},\n",
       " 'chang': {7: [6]},\n",
       " 'collabor': {7: [7]},\n",
       " 'debug': {8: [0]},\n",
       " 'process': {8: [1]},\n",
       " 'find': {8: [2]},\n",
       " 'fix': {8: [3]},\n",
       " 'error': {8: [4]},\n",
       " 'framework': {9: [1]},\n",
       " 'simplifi': {9: [2]},\n",
       " 'applic': {9: [5]},\n",
       " 'artifici': {10: [0]},\n",
       " 'intellig': {10: [1]},\n",
       " 'appli': {10: [2]},\n",
       " 'task': {10: [5]}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_positional_index(list_of_sentence_tokens):\n",
    "    ## TODO: Implement the functionality that will return the positional index\n",
    "    output = {}\n",
    "    \n",
    "    # Iterate through each sentence with its Index\n",
    "    for sentence_index, tokens in enumerate(list_of_sentence_tokens, start=1):\n",
    "        # Iterate through each token and its position in the sentence\n",
    "        for position, token in enumerate(tokens):\n",
    "            # Add the token to the positional index\n",
    "            if token not in output:\n",
    "                # If token not already in the index, initialize its entry\n",
    "                output[token] = {}\n",
    "            if sentence_index not in output[token]:\n",
    "                # If sentence ID not already in token's entry, initialize it\n",
    "                output[token][sentence_index] = []\n",
    "            # Append the position to the token's sentence entry\n",
    "            output[token][sentence_index].append(position)\n",
    "    \n",
    "    return output\n",
    "get_positional_index(clean_sentences(sample_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Question B-3: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the <b>TF-IDF Matrix</b> that is sufficient to represent the documents, the tokens are expected to be sorted as well as documentIDs. Assume that each sentence is a document and the sentence ID starts from 1. (10) You are not allowed to use any libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.256,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.256,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.256,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.102,\n",
       "  0.256,\n",
       "  0.358,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.179,\n",
       "  0.256,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.322,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.461,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.461,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.322,\n",
       "  0.461,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.576,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.576,\n",
       "  0.0,\n",
       "  0.576,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.576,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.201,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.384,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.268,\n",
       "  0.0,\n",
       "  0.153,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.268,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.384],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.461,\n",
       "  0.461,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.461,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.183,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.461,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.461,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.461,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.322,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.322,\n",
       "  0.461,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.288,\n",
       "  0.15,\n",
       "  0.288,\n",
       "  0.288,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.288,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.288,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.288,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.288,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.172,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.329,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.329,\n",
       "  0.329,\n",
       "  0.329,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.329,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.23,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.384,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.268,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.384,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.384,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.536,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.384,\n",
       "  0.0,\n",
       "  0.384,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.384,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.153,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.384,\n",
       "  0.0,\n",
       "  0.268,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "def get_TFIDF_matrix(list_of_sentence_tokens):\n",
    "    ## TODO: Implement the functionality that will return the tf-idf matrix\n",
    "\n",
    "    # Get all unique terms (vocabulary)\n",
    "    vocabulary = sorted(set(term for sentence in list_of_sentence_tokens for term in sentence))\n",
    "    # print(vocabulary)\n",
    "    # Count term frequencies per document\n",
    "    term_frequencies = []\n",
    "    for sentence in list_of_sentence_tokens:\n",
    "        term_count = {term: 0 for term in vocabulary}\n",
    "        for term in sentence:\n",
    "            term_count[term] += 1\n",
    "        term_frequencies.append(term_count)\n",
    "    # print(term_frequencies)\n",
    "\n",
    "    # Calculate document frequencies (DF) for each term\n",
    "    document_frequencies = {term: 0 for term in vocabulary}\n",
    "    for term in vocabulary:\n",
    "        for sentence in list_of_sentence_tokens:\n",
    "            if term in sentence:\n",
    "                document_frequencies[term] += 1\n",
    "    # print(document_frequencies)\n",
    "\n",
    "    # calculate IDF for each term\n",
    "    documents_count = len(list_of_sentence_tokens)\n",
    "    idf = {}\n",
    "    for term, df in document_frequencies.items():\n",
    "        idf[term] = math.log(documents_count / (df))\n",
    "    # print(idf)\n",
    "\n",
    "    # Compute TF-IDF\n",
    "    tf_idf_matrix = []\n",
    "    for term_count in term_frequencies:\n",
    "        total_terms = sum(term_count.values())\n",
    "        tf_idf_row = []\n",
    "        for term in vocabulary:\n",
    "            tf = term_count[term] / total_terms  # Term Frequency\n",
    "            tf_idf_row.append(round(tf * idf[term], 3))  # TF-IDF Score, rounded to 3 decimals\n",
    "        tf_idf_matrix.append(tf_idf_row)\n",
    "    # print(tf_idf_matrix)\n",
    "    \n",
    "    return tf_idf_matrix\n",
    "    # output =[[1.254,0,0,0.564,1.11],[2.12,1.254,0.564,0,0]]\n",
    "    # return  output #THIS IS A PLACEHOLDER FOR THE OUTPUT YOU NEED TO OVERWRITE\n",
    "\n",
    "get_TFIDF_matrix(clean_sentences(sample_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART C- Measuring Documents Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a method that takes as an input: (15)\n",
    " - a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences.\n",
    " - A method name: \"tfidf\", \"inverted\"\n",
    " - A Search Query\n",
    " - Return the rank of the sentences based on the given method and a query <br>\n",
    "\n",
    "***Hint: For inverted index we just want documents that have the query word/words, for tfidf you must show the ranking based on highest tfidf score***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 3, 9, 0, 1, 2, 5, 6, 7, 8]\n",
      "[0, 3, 4, 9, 1, 2, 5, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "def get_ranked_documents(list_of_sentence_tokens, method_name, search_query):\n",
    "    # TODO: Implement the functionality that returns the rank of the documents based on the method given and the search query\n",
    "    ## If the method is \"inverted\" then rank the documents based on the number of matching tokens \n",
    "    ## If the method is \"tfidf\" then use the tfidf score equation in slides and return ranking based on the score\n",
    "    ## The document with highest relevance should be ranked first\n",
    "    ## list method should return the index of the documents based on highest ranking first\n",
    "\n",
    "    rank_list = []\n",
    "    \n",
    "    # Get unique tokens\n",
    "    vocabulary = sorted(set(term for sentence in list_of_sentence_tokens for term in sentence))\n",
    "    \n",
    "    # Get TdIdf metrix from previous function\n",
    "    tf_idf_matrix = get_TFIDF_matrix(list_of_sentence_tokens) \n",
    "\n",
    "    # Normalize search query\n",
    "    search_tokens = search_query.split()\n",
    "    \n",
    "    if method_name == \"tfidf\":\n",
    "        # TF-IDF method: Rank based on TF-IDF score\n",
    "        document_scores = []\n",
    "        for i, sentence in enumerate(list_of_sentence_tokens):\n",
    "            score = 0\n",
    "            for token in search_tokens:\n",
    "                if token in vocabulary:\n",
    "                    score += tf_idf_matrix[i][vocabulary.index(token)]\n",
    "            document_scores.append((i, score))\n",
    "         # Sort by TF-IDF score in descending order\n",
    "        rank_list = [i for i, score in sorted(document_scores, key=lambda x: x[1], reverse=True)]\n",
    "    \n",
    "    elif method_name == \"inverted\":\n",
    "        # Inverted method: Rank based on token matching\n",
    "        document_scores = []\n",
    "        for i, sentence in enumerate(list_of_sentence_tokens):\n",
    "            match_count = sum(1 for token in search_tokens if token in sentence)\n",
    "            document_scores.append((i, match_count))\n",
    "        \n",
    "        # Sort by match count in descending order\n",
    "        rank_list = [i for i, score in sorted(document_scores, key=lambda x: x[1], reverse=True)]\n",
    "    \n",
    "    return rank_list\n",
    "\n",
    "print(get_ranked_documents(clean_sentences(sample_sentences), \"tfidf\", \"program\"))\n",
    "print(get_ranked_documents(clean_sentences(sample_sentences), \"inverted\", \"program\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART D- TFIDF with a TWIST (30 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TFIDF with Custom Weighting Based on Document Length and Term Position\n",
    "- You are expected to implement a twisted version of the TF-IDF vectorizer, that incorporates two additional features:\n",
    "    - Document Length\n",
    "    - Term Position\n",
    "- This twist aims to assign weight based on Modified Term Frequency (MTF) and Modified inverse Document Frequency (MIDF)\n",
    "1. Modified Term Frequency (MTF):\n",
    "    - MTF is calculated by taking into consideration the position of the term into account\n",
    "    - The assumption is the closer the term appears to the beginning of the document, the higher the weight should be.\n",
    "    - $$\\text{MTF}(t, d) = \\frac{f(t, d)}{1 + \\text{position}(t, d)}$$\n",
    "        - Where f(t,d) is the raw count of term t in document d.\n",
    "        - position(t,d) is the position of the first occurence of term t in document d.\n",
    "2. Modified Inverse Document Frequency (MIDF):\n",
    "    - MIDF is calculated taking into consideration the document length.\n",
    "    - The assumption is that the IDF should be inversely proportion not only to the number of documents it appears at, but also to the average length of documents where the term appears. \n",
    "    - Hence, longer documents are less significant for a term's relevance.\n",
    "    - $$\\text{MIDF}(t) = \\log \\left( \\frac{N}{\\text{df}(t) \\times \\frac{1}{M} \\sum_{d \\in D_{t}} |d|} \\right)$$\n",
    "\n",
    "        - N is the total number of documents\n",
    "        - df(t) is the document frequency\n",
    "        - M is a constant for scaling\n",
    "        - $${\\sum_{d \\in D_{t}} |d|}$$\n",
    "                 is the sum of the lengths of all documents that contain t\n",
    "        - |d| is the length of document d\n",
    "3. Final Weight (MTF-MIDF):\n",
    "    - The Combined is calculated as : MTF(t,d)*MIDF(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 4-A: Implement the function logic for getting modified tf-idf weightings. (20 Marks)\n",
    "<b><u>NOTE: M is a scaling factor, setting it to 5 in our example would be sufficient. However, you need to explore what does increasing and decreasing it represent.</u></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.011,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.013,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.021,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  -0.239,\n",
       "  0.015,\n",
       "  -0.47,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.045,\n",
       "  0.035,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.016,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.347,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.173,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.019,\n",
       "  0.231,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.183,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.458,\n",
       "  0.0,\n",
       "  0.305,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.229,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.148,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.17,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.014,\n",
       "  0.0,\n",
       "  -0.478,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  -0.016,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  0.128],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.173,\n",
       "  0.347,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  0.139,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  -0.159,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  0.231,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.347,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.173,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.016,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  -0.019,\n",
       "  0.231,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.028,\n",
       "  -0.106,\n",
       "  0.025,\n",
       "  0.074,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.045,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.037,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.056,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  0.112,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.093,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.178,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.059,\n",
       "  0.089,\n",
       "  0.071,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.119,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  -0.067,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.073,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.019,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.17,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.128,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.095,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.128,\n",
       "  0.0,\n",
       "  0.255,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.17,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  -0.159,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.073,\n",
       "  0.0,\n",
       "  -0.081,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  -0.0,\n",
       "  0.0,\n",
       "  0.0]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_mtf(term, doc):\n",
    "    # Calculate Modified Term Frequency (MTF) for a term in a document.\n",
    "    term_count = doc.count(term)\n",
    "    if term_count == 0:\n",
    "        return 0\n",
    "    first_position = doc.index(term) + 1\n",
    "    return term_count / (1 + first_position)\n",
    "\n",
    "def calculate_midf(term, list_of_docs, total_docs, M=1):\n",
    "    # Calculate Modified Inverse Document Frequency (MIDF) for a term across documents.\n",
    "    # Get all documents containing the term\n",
    "    docs_with_term = [doc for doc in list_of_docs if term in doc]\n",
    "    df = len(docs_with_term)\n",
    "    \n",
    "    if df == 0:\n",
    "        return 0\n",
    "\n",
    "    # Average length of documents containing the term\n",
    "    total_length = sum(len(doc) for doc in docs_with_term)\n",
    "    avg_length = total_length / len(docs_with_term)\n",
    "    \n",
    "    return math.log(total_docs / (df * (1 / M) * avg_length))\n",
    "\n",
    "\n",
    "def get_modified_tfidf_matrix(list_of_sentence_tokens):\n",
    "    ## TODO: Implement the functionality that will return the modified tf-idf matrix\n",
    "    M = 5\n",
    "\n",
    "    # Vocabulary\n",
    "    vocabulary = sorted(set(term for sentence in list_of_sentence_tokens for term in sentence))\n",
    "    total_docs = len(list_of_sentence_tokens)\n",
    "\n",
    "    # Calculate MIDF for each term\n",
    "    midf_scores = {}\n",
    "    for term in vocabulary:\n",
    "        midf_scores[term] = calculate_midf(term, list_of_sentence_tokens, total_docs)\n",
    "\n",
    "    # Calculate MTF-MIDF for each term in each document\n",
    "    output = []\n",
    "    for doc in list_of_sentence_tokens:\n",
    "        tfidf_row = []\n",
    "        for term in vocabulary:\n",
    "            mtf = calculate_mtf(term, doc)\n",
    "            midf = midf_scores[term]\n",
    "            tfidf_row.append(round(mtf * midf, 3))  # Combined weight\n",
    "        output.append(tfidf_row)\n",
    "\n",
    "    return  output #THIS IS A PLACEHOLDER FOR THE OUTPUT YOU NEED TO OVERWRITE\n",
    "\n",
    "\n",
    "get_modified_tfidf_matrix(clean_sentences(sample_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 4-B: Experiment the effect of changing M and comment on what do you think M is for and why is it added. (5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiments results for different M values\n",
    "After comparing results for different M values, Here is my understanding\n",
    "<br>\n",
    "<br>\n",
    "M is a scaling fector in Modified TF-IDF method, this helps to assign weights to terms as per document length. \n",
    "<br>\n",
    "<br>\n",
    "```M=1``` The impact of **document length is moderate** and well matched with the frequency of documents.\n",
    "<br>\n",
    "This acts as a starting point where the weight is equally distributed between document length and phrase frequency.\n",
    "<br>\n",
    "<br>\n",
    "```M<0``` The impact of **document length is higher**.\n",
    "<br>\n",
    "When documnets are longer, it reduces MIDF score, whhich reduces weight for terms\n",
    "<br>\n",
    "<br>\n",
    "```M>0``` The impact of **document length is lower** \n",
    "<br>\n",
    "Here, when term is repeated in multiple documents, it gains more weight. Here, document length has no or minor effect on score\n",
    "\n",
    "##### Purpose of M\n",
    "- M controls impact of document length over the MIDF score. \n",
    "- M ensures that the denominator doesn't become excessively small, which would lead to unrealistically high weights.\n",
    "- For datasets where document length strongly corresponds with noise or useless information, small M is helpful.\n",
    "- For datasets where document length has little bearing on relevance, large M is appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 4-C: Do you think Modified TF-Modified IDF is a good technique? Please comment and explain your thoughts.(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Advantages of Modified TF-MIDF\t| Disadvantages of Modified TF-MIDF |\n",
    "|-------------------------------|------------------------------------|\n",
    "|Considers Term Position: Prioritizes terms that appear earlier, which often enhances relevance in many contexts.|Favors Shorter Documents: Assigns lower weights to terms in lengthy documents, which may not always be appropriate.|\n",
    "|Accounts for Document Length: Adjusts weights to reduce the impact of verbose documents, improving performance in varied datasets.\t|Overemphasizes Position: Assumes that terms appearing earlier are always more significant, which might not apply to all types of text.|\n",
    "|Combines Local and Global Factors: Effectively balances term relevance within a document (MTF) and across documents (MIDF). | More Computationally Intensive: Introduces additional complexity due to position-based and document-length calculations.|\n",
    "| Customizable with 𝑀: Offers flexibility to adapt to different datasets and applications.\t| Limited Generalization: May not perform well in domains where long documents are highly informative.|\n",
    "| Enhances Ranking Relevance: Performs effectively in search engines and datasets with shorter text. | Resource Demanding: The added computational requirements can be a bottleneck for large-scale datasets.|\n",
    "\n",
    "By accounting for term position and document length, the Modified TF-MIDF approach improves upon the regular TF-IDF and is appropriate for situations in which these factors significantly affect relevance. It is useful for particular applications like search engines or short-text analysis because of its flexibility with M and capacity to fine-tune ranking. Its job in broader contexts may not always be justified, though, due to its additional complexity and certain biases towards shorter publications. When customised to the features of a particular dataset or task, it works best."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvPROG8245",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
